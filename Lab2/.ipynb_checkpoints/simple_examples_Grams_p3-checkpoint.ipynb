{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, bigrams, and trigrams in Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ngrams_bayes():\n",
    "    \n",
    "    def __init__(self,data,n=2,split=.75,labels=None,train_data=None,test_data=None):\n",
    "        \n",
    "        #Setting the labels of classes\n",
    "        self.labels = labels\n",
    "        \n",
    "        #If the training and test datasets are passed as parameters\n",
    "        if train_data is not None: self.train_data = train_data\n",
    "        if test_data is not None: self.test_data = test_data\n",
    "        \n",
    "        #If the training and test datasets are not set\n",
    "        if train_data is None and test_data is None:\n",
    "            #split into training and testing data\n",
    "            self.train_data, self.test_data = train_test_split(data, \n",
    "                                                               train_size = split)\n",
    "        \n",
    "        #convert into n grams\n",
    "        self.train_data = [[item[0], self.ngrams(n, item[1])] for item in self.train_data]\n",
    "        self.test_data = [[item[0], self.ngrams(n, item[1])] for item in self.test_data]\n",
    "        \n",
    "        #count unique n grams in training data\n",
    "        flattened = [gram for message in self.train_data for gram in message[1]]\n",
    "        self.unique = len(set(flattened))\n",
    "        \n",
    "        #init dicts\n",
    "        self.trainPositive = {}\n",
    "        self.trainNegative = {}\n",
    "        #counters \n",
    "        self.posGramCount = 0\n",
    "        self.negGramCount = 0\n",
    "        self.spamCount = 0\n",
    "        #priors\n",
    "        self.pA = 0\n",
    "        self.pNotA = 0\n",
    "    \n",
    "    def ngrams(self,n,text):\n",
    "        text = text.split(' ')\n",
    "        grams = []\n",
    "        for i in range(len(text)-n+1):\n",
    "            gram = ' '.join(text[i:i+n])\n",
    "            grams.append(gram)\n",
    "        return grams\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        for item in self.train_data:\n",
    "            label = item[0]\n",
    "            grams = item[1]\n",
    "            if label == 1:\n",
    "                self.spamCount +=1\n",
    "            for gram in grams:\n",
    "                if label == 1:\n",
    "                    self.trainPositive[gram] = self.trainPositive.get(gram,0) + 1\n",
    "                    self.posGramCount += 1\n",
    "                else:\n",
    "                    self.trainNegative[gram] = self.trainNegative.get(gram,0) + 1\n",
    "                    self.negGramCount += 1\n",
    "            \n",
    "            self.pA = self.spamCount / float(len(self.train_data))\n",
    "            self.pNotA = 1.0 - self.pA\n",
    "    \n",
    "    def classify(self,text,alpha=1.0):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        isSpam = self.pA * self.conditionalText(text,1)\n",
    "        notSpam = self.pNotA * self.conditionalText(text,0)\n",
    "        if(isSpam > notSpam):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def probabilities(self,text,alpha=1.0):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        isSpam = self.pA * self.conditionalText(text,1)\n",
    "        notSpam = self.pNotA * self.conditionalText(text,0)\n",
    "        return isSpam/(isSpam+notSpam),notSpam/(isSpam+notSpam)\n",
    "        \n",
    "    def conditionalText(self,grams,label):\n",
    "        result = 1.0\n",
    "        for ngram in grams:\n",
    "            result *= self.conditionalNgram(ngram,label)\n",
    "        return result\n",
    "    \n",
    "    def conditionalNgram(self,ngram,label):\n",
    "        alpha = self.alpha\n",
    "        if label == 1:\n",
    "            return ((self.trainPositive.get(ngram,0)+alpha)/\n",
    "                    float(self.posGramCount+alpha*self.unique))\n",
    "        \n",
    "        else:\n",
    "            return ((self.trainNegative.get(ngram,0)+alpha)/\n",
    "                    float(self.negGramCount+alpha*self.unique))\n",
    "    \n",
    "    def evaluate_test_data(self):\n",
    "        results = []\n",
    "        for test in self.test_data:\n",
    "            label = test[0]\n",
    "            text = test[1]\n",
    "            ruling = self.classify(text)\n",
    "            if ruling == label:\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "        print(\"Probabilities for {} classes: {:.2f}%\".format(len(results),100.0*sum(results)/\n",
    "    float(len(results))))\n",
    "        return sum(results)/float(len(results))\n",
    "    \n",
    "    def evaluate_test_data_prob(self):\n",
    "        results = []\n",
    "        for test in self.test_data:\n",
    "            label = test[0]\n",
    "            text = test[1]\n",
    "            class0,class1 = self.probabilities(text)\n",
    "            if class0>class1: index = 0\n",
    "            else: index = 1\n",
    "            print(\"Probabilities for classes: \" + str(class0) + \", \" + str(class1))\n",
    "            print(\"The sample \\\"\" + str(test) + \"\\\" belongs to class \" + str(self.labels[index]) )\n",
    "        return np.array([class0,class1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for the example 13.1\n",
      "[[1 'Chinese Beijing Chinese']\n",
      " [1 'Chinese Chinese Shanghai']\n",
      " [1 'Chinese Macao']\n",
      " [0 'Tokyo Japan Chinese']\n",
      " ['?' 'Chinese Chinese Chinese Tokyo Japan']]\n",
      "\n",
      " Data for the example 13.10\n",
      "[[1 'Taipei Taiwan']\n",
      " [1 'Macao Taiwan Shanghai']\n",
      " [0 'Japan Sapporo']\n",
      " [0 'Sapporo Osaka']\n",
      " ['?' 'Taiwan Taiwan Sapporo']]\n",
      "\n",
      " Data for the example spam detection\n",
      "[[1 'send us your password']\n",
      " [0 'send us your review']\n",
      " [0 'review your password']\n",
      " [1 'review us']\n",
      " [1 'send your password']\n",
      " [1 'send us your password']\n",
      " ['?' 'review us now']]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"example_131.csv\",delimiter=\",\",names = ['label','message'],encoding = 'latin-1') #Loading the data from example 13.1\n",
    "df2 = pd.read_csv(\"example_1310.csv\",delimiter=\",\",names = ['label','message'],encoding = 'latin-1') #Loading the data from example 13.10\n",
    "df3 = pd.read_csv(\"example_spam.csv\",delimiter=\",\",names = ['label','message'],encoding = 'latin-1') #Loading the data from example spam detection\n",
    "\n",
    "#label China as 1, not China as 0\n",
    "df['label'] = df['label'].replace(['Not China','China'],[0,1])\n",
    "#label China as 1, not China as 0\n",
    "df2['label'] = df2['label'].replace(['Not China','China'],[0,1])\n",
    "#label spam as 1, not spam as 0\n",
    "df3['label'] = df3['label'].replace(['ham','spam'],[0,1])\n",
    "\n",
    "data = df.values\n",
    "data2 = df2.values\n",
    "data3 = df3.values\n",
    "\n",
    "print(\"Data for the example 13.1\")\n",
    "print(data)\n",
    "print(\"\\n Data for the example 13.10\")\n",
    "print(data2)\n",
    "print(\"\\n Data for the example spam detection\")\n",
    "print(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test datasets for the example 13.1\n",
      "Train dataset\n",
      "[[1 'Chinese Beijing Chinese']\n",
      " [1 'Chinese Chinese Shanghai']\n",
      " [1 'Chinese Macao']\n",
      " [0 'Tokyo Japan Chinese']]\n",
      "Test dataset\n",
      "[['?' 'Chinese Chinese Chinese Tokyo Japan']]\n",
      "\n",
      "Training and test datasets for the example 13.1\n",
      "Train dataset\n",
      "[[1 'Taipei Taiwan']\n",
      " [1 'Macao Taiwan Shanghai']\n",
      " [0 'Japan Sapporo']\n",
      " [0 'Sapporo Osaka']]\n",
      "Test dataset\n",
      "[['?' 'Taiwan Taiwan Sapporo']]\n",
      "\n",
      "Training and test datasets for the example 13.1\n",
      "Train dataset\n",
      "[[1 'send us your password']\n",
      " [0 'send us your review']\n",
      " [0 'review your password']\n",
      " [1 'review us']\n",
      " [1 'send your password']\n",
      " [1 'send us your password']]\n",
      "Test dataset\n",
      "[['?' 'review us now']]\n"
     ]
    }
   ],
   "source": [
    "#For example 13.1\n",
    "x_train = df.get_values()[0:4]\n",
    "x_test = df.get_values()[4:5]\n",
    "print(\"Training and test datasets for the example 13.1\")\n",
    "print(\"Train dataset\")\n",
    "print(x_train)\n",
    "print(\"Test dataset\")\n",
    "print(x_test)\n",
    "\n",
    "#For example 13.10\n",
    "x_train2 = df2.get_values()[0:4]\n",
    "x_test2 = df2.get_values()[4:5]\n",
    "print(\"\\nTraining and test datasets for the example 13.1\")\n",
    "print(\"Train dataset\")\n",
    "print(x_train2)\n",
    "print(\"Test dataset\")\n",
    "print(x_test2)\n",
    "\n",
    "#For example spam detection\n",
    "x_train3 = df3.get_values()[0:6]\n",
    "x_test3 = df3.get_values()[6:7]\n",
    "print(\"\\nTraining and test datasets for the example 13.1\")\n",
    "print(\"Train dataset\")\n",
    "print(x_train3)\n",
    "print(\"Test dataset\")\n",
    "print(x_test3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the ngrams_bayes class for each dataset, unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for the example 13.1\n",
      "Probabilities for classes: 0.6897586117634673, 0.31024138823653263\n",
      "The sample \"['?', ['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']]\" belongs to class China\n",
      "\n",
      "Data for the example 13.10\n",
      "Probabilities for classes: 0.697954902988988, 0.30204509701101206\n",
      "The sample \"['?', ['Taiwan', 'Taiwan', 'Sapporo']]\" belongs to class China\n",
      "\n",
      "Data for the example spam detection\n",
      "Probabilities for classes: 0.4413793103448275, 0.5586206896551725\n",
      "The sample \"['?', ['review', 'us', 'now']]\" belongs to class spam\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.44137931, 0.55862069])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Data for the example 13.1\")\n",
    "unigram_bayes = ngrams_bayes(data,1,labels=['China','Not China'],train_data = x_train,test_data = x_test)\n",
    "unigram_bayes.train()\n",
    "unigram_bayes.evaluate_test_data_prob()\n",
    "\n",
    "print(\"\\nData for the example 13.10\")\n",
    "unigram_bayes2 = ngrams_bayes(data2,1,labels=['China','Not China'],train_data = x_train2,test_data = x_test2)\n",
    "unigram_bayes2.train()\n",
    "unigram_bayes2.evaluate_test_data_prob()\n",
    "\n",
    "print(\"\\nData for the example spam detection\")\n",
    "unigram_bayes3 = ngrams_bayes(data3,1,labels=['ham','spam'],train_data = x_train3,test_data = x_test3)\n",
    "unigram_bayes3.train()\n",
    "unigram_bayes3.evaluate_test_data_prob()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the ngrams_bayes class for each dataset, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for the example 13.1\n",
      "Probabilities for classes: 0.6549865229110512, 0.34501347708894875\n",
      "The sample \"['?', ['Chinese Chinese', 'Chinese Chinese', 'Chinese Tokyo', 'Tokyo Japan']]\" belongs to class China\n",
      "\n",
      "Data for the example 13.10\n",
      "Probabilities for classes: 0.43362831858407086, 0.5663716814159292\n",
      "The sample \"['?', ['Taiwan Taiwan', 'Taiwan Sapporo']]\" belongs to class Not China\n",
      "\n",
      "Data for the example spam detection\n",
      "Probabilities for classes: 0.6923076923076922, 0.3076923076923077\n",
      "The sample \"['?', ['review us', 'us now']]\" belongs to class ham\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.69230769, 0.30769231])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Data for the example 13.1\")\n",
    "unigram_bayes = ngrams_bayes(data,2,labels=['China','Not China'],train_data = x_train,test_data = x_test)\n",
    "unigram_bayes.train()\n",
    "unigram_bayes.evaluate_test_data_prob()\n",
    "\n",
    "print(\"\\nData for the example 13.10\")\n",
    "unigram_bayes2 = ngrams_bayes(data2,2,labels=['China','Not China'],train_data = x_train2,test_data = x_test2)\n",
    "unigram_bayes2.train()\n",
    "unigram_bayes2.evaluate_test_data_prob()\n",
    "\n",
    "print(\"\\nData for the example spam detection\")\n",
    "unigram_bayes3 = ngrams_bayes(data3,2,labels=['ham','spam'],train_data = x_train3,test_data = x_test3)\n",
    "unigram_bayes3.train()\n",
    "unigram_bayes3.evaluate_test_data_prob()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
