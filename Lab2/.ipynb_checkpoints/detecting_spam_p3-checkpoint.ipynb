{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ham/Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"example_131.csv\",delimiter=\",\",names = ['label','message']) #Loading the data from example 13.1\n",
    "df2 = pd.read_csv(\"example_1310.csv\",delimiter=\",\",names = ['label','message']) #Loading the data from example 13.10\n",
    "df3 = pd.read_csv(\"example_spam.csv\",delimiter=\",\",names = ['label','message']) #Loading the data from example spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages for the example 13.1\n",
      "0                Chinese Beijing Chinese\n",
      "1               Chinese Chinese Shanghai\n",
      "2                          Chinese Macao\n",
      "3                    Tokyo Japan Chinese\n",
      "4    Chinese Chinese Chinese Tokyo Japan\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages for the example 13.10\n",
      "0            Taipei Taiwan\n",
      "1    Macao Taiwan Shanghai\n",
      "2            Japan Sapporo\n",
      "3            Sapporo Osaka\n",
      "4    Taiwan Taiwan Sapporo\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages for the example spam detection\n",
      "0    send us your password\n",
      "1      send us your review\n",
      "2     review your password\n",
      "3                review us\n",
      "4       send your password\n",
      "5    send us your password\n",
      "6            review us now\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "values,values2,values3 = df.get_values(),df2.get_values(),df3.get_values()\n",
    "print(\"Messages for the example 13.1\")\n",
    "print(df['message'])\n",
    "print(\"\\n Messages for the example 13.10\")\n",
    "print(df2['message'])\n",
    "print(\"\\n Messages for the example spam detection\")\n",
    "print(df3['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing labels by numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for the example 13.1\n",
      "(5, 2)\n",
      "[['China' 'Chinese Beijing Chinese']\n",
      " ['China' 'Chinese Chinese Shanghai']\n",
      " ['China' 'Chinese Macao']\n",
      " ['Not China' 'Tokyo Japan Chinese']\n",
      " ['?' 'Chinese Chinese Chinese Tokyo Japan']]\n",
      "\n",
      " Values for the example 13.10\n",
      "(5, 2)\n",
      "[['China' 'Taipei Taiwan']\n",
      " ['China' 'Macao Taiwan Shanghai']\n",
      " ['Not China' 'Japan Sapporo']\n",
      " ['Not China' 'Sapporo Osaka']\n",
      " ['?' 'Taiwan Taiwan Sapporo']]\n",
      "\n",
      " Values for the example spam detection\n",
      "(7, 2)\n",
      "[['spam' 'send us your password']\n",
      " ['ham' 'send us your review']\n",
      " ['ham' 'review your password']\n",
      " ['spam' 'review us']\n",
      " ['spam' 'send your password']\n",
      " ['spam' 'send us your password']\n",
      " ['?' 'review us now']]\n"
     ]
    }
   ],
   "source": [
    "#df['label'] = df.label.map({'0':0,'1':1})\n",
    "values = df.get_values()\n",
    "print(\"Values for the example 13.1\")\n",
    "print(values.shape)\n",
    "print(values)\n",
    "print(\"\\n Values for the example 13.10\")\n",
    "print(values2.shape)\n",
    "print(values2)\n",
    "print(\"\\n Values for the example spam detection\")\n",
    "print(values3.shape)\n",
    "print(values3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert all characters in the message to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages in lower-case for the example 13.1\n",
      "0                chinese beijing chinese\n",
      "1               chinese chinese shanghai\n",
      "2                          chinese macao\n",
      "3                    tokyo japan chinese\n",
      "4    chinese chinese chinese tokyo japan\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages in lower-case for the example 13.10\n",
      "0            taipei taiwan\n",
      "1    macao taiwan shanghai\n",
      "2            japan sapporo\n",
      "3            sapporo osaka\n",
      "4    taiwan taiwan sapporo\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages in lower-case for the example spam detection\n",
      "0    send us your password\n",
      "1      send us your review\n",
      "2     review your password\n",
      "3                review us\n",
      "4       send your password\n",
      "5    send us your password\n",
      "6            review us now\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['message'] = df['message'].str.lower()\n",
    "df2['message'] = df2['message'].str.lower()\n",
    "df3['message'] = df3['message'].str.lower()\n",
    "print(\"Messages in lower-case for the example 13.1\")\n",
    "print(df['message'])\n",
    "print(\"\\n Messages in lower-case for the example 13.10\")\n",
    "print(df2['message'])\n",
    "print(\"\\n Messages in lower-case for the example spam detection\")\n",
    "print(df3['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to import and download the tokenizer from the console:\n",
    "An installation window will appear. Go to the \"Models\" tab and select \"punkt\" from the \"Identifier\" column. Then click \"Download\" and it will install the necessary files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Messages after tokenization for the example 13.10\n",
      "0                  [chinese, beijing, chinese]\n",
      "1                 [chinese, chinese, shanghai]\n",
      "2                             [chinese, macao]\n",
      "3                      [tokyo, japan, chinese]\n",
      "4    [chinese, chinese, chinese, tokyo, japan]\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages after tokenization for the example 13.1\n",
      "0             [taipei, taiwan]\n",
      "1    [macao, taiwan, shanghai]\n",
      "2             [japan, sapporo]\n",
      "3             [sapporo, osaka]\n",
      "4    [taiwan, taiwan, sapporo]\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages after tokenization for the example spam detection\n",
      "0    [send, us, your, password]\n",
      "1      [send, us, your, review]\n",
      "2      [review, your, password]\n",
      "3                  [review, us]\n",
      "4        [send, your, password]\n",
      "5    [send, us, your, password]\n",
      "6             [review, us, now]\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['message'] = df['message'].apply(nltk.word_tokenize)\n",
    "df2['message'] = df2['message'].apply(nltk.word_tokenize)\n",
    "df3['message'] = df3['message'].apply(nltk.word_tokenize)\n",
    "print(\"\\n Messages after tokenization for the example 13.10\")\n",
    "print(df['message'])\n",
    "print(\"\\n Messages after tokenization for the example 13.1\")\n",
    "print(df2['message'])\n",
    "print(\"\\n Messages after tokenization for the example spam detection\")\n",
    "print(df3['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming the messages using the Porter Stemmer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages after stemming for the example 13.1\n",
      "0                    [chines, beij, chines]\n",
      "1                [chines, chines, shanghai]\n",
      "2                           [chines, macao]\n",
      "3                    [tokyo, japan, chines]\n",
      "4    [chines, chines, chines, tokyo, japan]\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages after stemming for the example 13.10\n",
      "0             [taipei, taiwan]\n",
      "1    [macao, taiwan, shanghai]\n",
      "2             [japan, sapporo]\n",
      "3             [sapporo, osaka]\n",
      "4    [taiwan, taiwan, sapporo]\n",
      "Name: message, dtype: object\n",
      "\n",
      " Messages after stemming for the example spam detection\n",
      "0    [send, us, your, password]\n",
      "1      [send, us, your, review]\n",
      "2      [review, your, password]\n",
      "3                  [review, us]\n",
      "4        [send, your, password]\n",
      "5    [send, us, your, password]\n",
      "6             [review, us, now]\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df2['message'] = df2['message'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df3['message'] = df3['message'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "print(\"Messages after stemming for the example 13.1\")\n",
    "print(df['message'])\n",
    "print(\"\\n Messages after stemming for the example 13.10\")\n",
    "print(df2['message'])\n",
    "print(\"\\n Messages after stemming for the example spam detection\")\n",
    "print(df3['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming data into occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts and messages for the example 13.1\n",
      "[[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1]\n",
      " [0 3 1 0 0 1]]\n",
      "0                  chines beij chines\n",
      "1              chines chines shanghai\n",
      "2                        chines macao\n",
      "3                  tokyo japan chines\n",
      "4    chines chines chines tokyo japan\n",
      "Name: message, dtype: object\n",
      "\n",
      " Counts and messages for the example 13.10\n",
      "[[0 0 0 0 0 1 1]\n",
      " [0 1 0 0 1 0 1]\n",
      " [1 0 0 1 0 0 0]\n",
      " [0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 2]]\n",
      "0            taipei taiwan\n",
      "1    macao taiwan shanghai\n",
      "2            japan sapporo\n",
      "3            sapporo osaka\n",
      "4    taiwan taiwan sapporo\n",
      "Name: message, dtype: object\n",
      "\n",
      " Counts and messages for the example spam detection\n",
      "[[0 1 0 1 1 1]\n",
      " [0 0 1 1 1 1]\n",
      " [0 1 1 0 0 1]\n",
      " [0 0 1 0 1 0]\n",
      " [0 1 0 1 0 1]\n",
      " [0 1 0 1 1 1]\n",
      " [1 0 1 0 1 0]]\n",
      "0    send us your password\n",
      "1      send us your review\n",
      "2     review your password\n",
      "3                review us\n",
      "4       send your password\n",
      "5    send us your password\n",
      "6            review us now\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
    "df2['message'] = df2['message'].apply(lambda x: ' '.join(x))\n",
    "df3['message'] = df3['message'].apply(lambda x: ' '.join(x))\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(df['message'])\n",
    "counts2 = count_vect.fit_transform(df2['message'])\n",
    "counts3 = count_vect.fit_transform(df3['message'])\n",
    "\n",
    "print(\"Counts and messages for the example 13.1\")\n",
    "print(counts.toarray())\n",
    "print(df['message'])\n",
    "print(\"\\n Counts and messages for the example 13.10\")\n",
    "print(counts2.toarray())\n",
    "print(df2['message'])\n",
    "print(\"\\n Counts and messages for the example spam detection\")\n",
    "print(counts3.toarray())\n",
    "print(df3['message'])\n",
    "#print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts after transforming for the example 13.1\n",
      "[[0.72391022 0.68989419 0.         0.         0.         0.        ]\n",
      " [0.         0.68989419 0.         0.         0.72391022 0.        ]\n",
      " [0.         0.43016528 0.         0.90275015 0.         0.        ]\n",
      " [0.         0.38537163 0.65249088 0.         0.         0.65249088]\n",
      " [0.         0.78157088 0.44110484 0.         0.         0.44110484]]\n",
      "\n",
      " Counts after transforming for the example 13.10\n",
      "[[0.         0.         0.         0.         0.         0.83088075\n",
      "  0.55645052]\n",
      " [0.         0.63907044 0.         0.         0.63907044 0.\n",
      "  0.42799292]\n",
      " [0.83088075 0.         0.         0.55645052 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.83088075 0.55645052 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.4472136  0.         0.\n",
      "  0.89442719]]\n",
      "\n",
      " Counts after transforming for the example spam detection\n",
      "[[0.         0.53189578 0.         0.53189578 0.46592583 0.46592583]\n",
      " [0.         0.         0.53189578 0.53189578 0.46592583 0.46592583]\n",
      " [0.         0.60113188 0.60113188 0.         0.         0.52657472]\n",
      " [0.         0.         0.75221423 0.         0.65891863 0.        ]\n",
      " [0.         0.60113188 0.         0.60113188 0.         0.52657472]\n",
      " [0.         0.53189578 0.         0.53189578 0.46592583 0.46592583]\n",
      " [0.77366919 0.         0.47659523 0.         0.4174841  0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fercho\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer().fit(counts)\n",
    "counts = transformer.transform(counts)\n",
    "transformer2 = TfidfTransformer().fit(counts2)\n",
    "counts2 = transformer2.transform(counts2)\n",
    "transformer3 = TfidfTransformer().fit(counts3)\n",
    "counts3 = transformer3.transform(counts3)\n",
    "\n",
    "print(\"Counts after transforming for the example 13.1\")\n",
    "print(counts.toarray())\n",
    "print(\"\\n Counts after transforming for the example 13.10\")\n",
    "print(counts2.toarray())\n",
    "print(\"\\n Counts after transforming for the example spam detection\")\n",
    "print(counts3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(counts, df['label'], test_size=.1)\n",
    "x_train = counts[:4]\n",
    "x_test = counts[4:5]\n",
    "y_train = pd.Series(df.get_values()[0:4][:,0])\n",
    "y_test = pd.Series(df.get_values()[4:5][:,0])\n",
    "\n",
    "x_train2 = counts2[:4]\n",
    "x_test2 = counts2[4:5]\n",
    "y_train2 = pd.Series(df2.get_values()[0:4][:,0])\n",
    "y_test2 = pd.Series(df2.get_values()[4:5][:,0])\n",
    "\n",
    "x_train3 = counts3[:6]\n",
    "x_test3 = counts3[6:7]\n",
    "y_train3 = pd.Series(df3.get_values()[0:6][:,0])\n",
    "y_test3 = pd.Series(df3.get_values()[6:7][:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initializing Multinomial Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: ['China']\n",
      "Probabilities for each class: [[0.67804117 0.32195883]]\n",
      "Predicted class: ['China']\n",
      "Probabilities for each class: [[0.55857543 0.44142457]]\n",
      "Predicted class: ['spam']\n",
      "Probabilities for each class: [[0.41671911 0.58328089]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "predicted = model.predict(x_test)\n",
    "print(\"Predicted class: \" + str(predicted))\n",
    "print(\"Probabilities for each class: \" + str(model.predict_proba(x_test)))\n",
    "\n",
    "model.fit(x_train2, y_train2)\n",
    "predicted2 = model.predict(x_test2)\n",
    "print(\"Predicted class: \" + str(predicted2))\n",
    "print(\"Probabilities for each class: \" + str(model.predict_proba(x_test2)))\n",
    "\n",
    "model.fit(x_train3, y_train3)\n",
    "predicted3 = model.predict(x_test3)\n",
    "print(\"Predicted class: \" + str(predicted3))\n",
    "print(\"Probabilities for each class: \" + str(model.predict_proba(x_test3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
